{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many values are missing in the category_desc column\n",
    "print(volunteer['category_desc'].isnull().sum())\n",
    "\n",
    "# Subset the volunteer dataset\n",
    "volunteer_subset = volunteer[volunteer['category_desc'].notnull()]\n",
    "\n",
    "# Print out the shape of the subset\n",
    "print(volunteer_subset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the variance of the Proline column\n",
    "print(wine['Proline'].var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine['Proline_log'] = np.log(wine['Proline'])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(wine['Proline_log'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Take a subset of the DataFrame you want to scale \n",
    "wine_subset = wine[['Ash','Alcalinity of ash','Magnesium']]\n",
    "\n",
    "# Apply the scaler to the DataFrame subset\n",
    "wine_subset_scaled = ss.fit_transform(wine_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking[['Accessible', 'Accessible_enc']].head())\n",
    "\n",
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(volunteer['category_desc'])\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "print(category_enc.head())\n",
    "\n",
    "\n",
    "# Create a list of the columns to average\n",
    "run_columns = [\"run1\", \"run2\", \"run3\", \"run4\", \"run5\"]\n",
    "\n",
    "# Use apply to create a mean column\n",
    "running_times_5k[\"mean\"] = running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1)\n",
    "\n",
    "# Take a look at the results\n",
    "print(running_times_5k)\n",
    "\n",
    "\n",
    "# First, convert string column to date column\n",
    "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer['start_date_date'])\n",
    "\n",
    "# Extract just the month from the converted column\n",
    "volunteer[\"start_date_month\"] = volunteer['start_date_converted'].apply(lambda row: row.month)\n",
    "\n",
    "# Take a look at the converted and new month columns\n",
    "print(volunteer[['start_date_converted', 'start_date_month']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    pattern = re.compile(r\"\\d+\\.\\d+\")\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.match(pattern,length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking['Length'].apply(lambda row: return_mileage(row))\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the title text\n",
    "title_text = volunteer['title']\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train,y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the rest of the parameters\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Let's transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Let's sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, vector_index=8, top_n=3))\n",
    "\n",
    "\n",
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_,text_tfidf, 3)\n",
    "\n",
    "# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]\n",
    "\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc, using the filtered_text vector\n",
    "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(),y, stratify=y)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(train_X,train_y)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(test_X,test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
